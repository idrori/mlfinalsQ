{
       "Semester": "Fall 2019",
       "Question Number": "5",
       "Part": "c",
       "Points": 2.0,
       "Topic": "MDPs",
       "Type": "Image",
       "Question": "Consider the following simple MDP: Positive Reward\nFirst consider the case where the MDP has positive reward. In this scenario, there is only one action $($ next $)$; we name this decision policy $\\pi_{A}$ with $\\pi_{A}(s)=n e x t$ for all $s$. The reward is $R(s, n e x t)=0$ for all states $s$, except for state $s_{k}$ where reward is $R\\left(s_{k}\\right.$, next $)=10$. We always start at state $s_{1}$ and each arrow indicates a deterministic transition probability $p=1$. There is no transition out of the end state $E N D$, and 0 reward for any action from the end state. \nDerive a formula for $V_{\\pi}\\left(s_{1}\\right)$ that works for any value of (is expressed as a function of) $k$ and $\\gamma$ for the above positive reward MDP, in the infinite horizon case.",
       "Solution": "At each step, we receive a reward of 0 , except after the $k^{\\text {th }}$ step, when we get a reward of 10 . Therefore, the summation is\n$$\n\\sum_{i=0}^{k-1} 0 * \\gamma^{i}+10 * \\gamma^{k-1}=0 * \\gamma^{0}+0 * \\gamma^{1}+0 * \\gamma^{2}+0 * \\gamma^{3}+\\ldots+10 * \\gamma^{k-1}=10 \\gamma^{k-1}\n$$"
}