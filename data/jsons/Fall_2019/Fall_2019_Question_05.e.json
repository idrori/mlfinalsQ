{
       "Semester": "Fall 2019",
       "Question Number": "5",
       "Part": "e",
       "Points": 2.0,
       "Topic": "MDPs",
       "Type": "Image",
       "Question": "Consider the following simple MDP: Positive Reward\nFirst consider the case where the MDP has positive reward. In this scenario, there is only one action $($ next $)$; we name this decision policy $\\pi_{A}$ with $\\pi_{A}(s)=n e x t$ for all $s$. The reward is $R(s, n e x t)=0$ for all states $s$, except for state $s_{k}$ where reward is $R\\left(s_{k}\\right.$, next $)=10$. We always start at state $s_{1}$ and each arrow indicates a deterministic transition probability $p=1$. There is no transition out of the end state $E N D$, and 0 reward for any action from the end state.\nNow consider the case where this MDP has negative reward. In this scenario, the reward is $R(s, n e x t)=-1$ for all states, except for state $s_{k}$ where the reward is $R\\left(s_{k}, n e x t\\right)=0$. Again, there is only one action, next, and the decision policy remains $\\pi_{A}(s)=n e x t$ for all s. We always start at state $s_{1}$ and each arrow has a deterministic transition probsbility $p=1$. There is no transition out of the end state $E N D$, and zero reward for any action from the end state, i.e., $R(E N D$, next $)=0$.\nCalculate $V_{\\pi}(s)$ for each state in the infinite horizon case with $k=4$ and discount factor $\\gamma=0.9$",
       "Solution": "$$\n\\begin{aligned}\n&V_{\\pi}\\left(s_{4}\\right)=0 \\\\\n&V_{\\pi}\\left(s_{3}\\right)=-1+\\gamma * 0=-1 \\\\\n&V_{\\pi}\\left(s_{2}\\right)=-1+0.9(-1)=-1.9 \\\\\n&V_{\\pi}\\left(s_{1}\\right)=-1+0.9(-1.9)=-2.71\n\\end{aligned}\n$$"
}