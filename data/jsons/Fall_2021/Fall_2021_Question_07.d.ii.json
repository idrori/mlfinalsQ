{
       "Semester": "Fall 2021",
       "Question Number": "7",
       "Part": "d.ii",
       "Points": 1.0,
       "Topic": "Neural Networks",
       "Type": "Text",
       "Question": "A neural network is given as Z1 = X * W1, A1 = f1(Z1), Z2 = W2 * A1, \\hat{y} = f2(Z2). We now use back-propagation to update the weights during each iteration. Assume that we only have one data point (X, y) available to use, and the stepsize\nparameter is 0.01. Assume $X = [0, 0, 0, 0]^T, y = [0]$. Further assume that we start off with $W^1$ and $W^2$ as matrices/vectors of all ones. How many components of $W^1$ will get updated (i.e. have their value changed) after one iteration of back-propagation?",
       "Solution": "Zero Components"
}