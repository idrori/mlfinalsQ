{
       "Semester": "Fall 2017",
       "Question Number": "8",
       "Part": "c",
       "Points": 3.0,
       "Topic": "RNNs",
       "Type": "Image",
       "Question": "Consider three RNN variants:\n1. The basic RNN architecture we studied was\n$$\n\\begin{aligned}\n&s_{t}=f\\left(W^{s s} s_{t-1}+W^{s x} x_{t}\\right) \\\\\n&y_{t}=W^{o} s_{t}\n\\end{aligned}\n$$\nwhere $W^{s s}$ is $m \\times m, W^{s I}$ is $m \\times d$, and $f$ is an activation function to be specified later. We omit the offset parameters for simplicity (set them to zero).\n2. Ranndy thinks the basic RNN is representationally weak, and it would be better not to decompose the state update in this way. Ranndy's proposal is to instead\n$$\n\\begin{aligned}\n&s_{t}=f\\left(W^{s s x} \\operatorname{concat}\\left(s_{t-1}, x_{t}\\right)\\right) \\\\\n&y_{t}=W^{o} s_{t}\n\\end{aligned}\n$$\nwhere concat $\\left(s_{t-1}, x_{t}\\right)$ is a vector of length $m+d$ obtained by concatenating $s_{t-1}$ and $x_{t}$, so $W^{\\text {sss }}$ has dimensions $m \\times(m+d)$.\n3. Orenn wants to try yet another model, of the form:\n$$\n\\begin{aligned}\n&s_{t}=f\\left(W^{s s} s_{t-1}\\right)+f\\left(W^{s x} x_{t}\\right) \\\\\n&y_{t}=W^{o} s_{t}\n\\end{aligned}\n$$\nLec Surer insists on understanding these models a bit better, and how they might relate.\n\nAssume again that $f(z)=\\tanh (z)$. Suppose $s_{0}=0$ (vector) and we feed $x_{1}, \\ldots, x_{n}$ as the input sequence to Orenn's model, obtaining $y_{1}, \\ldots, y_{n}$ as the associated output sequence. If we change the input sequence to $-x_{1}, \\ldots,-x_{n}$, which of the following is the best characterization of the resulting output sequence?\n\nThe new output sequence will alternate between positive and negative values.\nThe new output sequence depends on the parameters.\nThe new output is just the negative of the previous output sequence\n",
       "Solution": "The new output is just the negative of the previous output sequence"
}